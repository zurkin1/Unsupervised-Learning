{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this kernel\n",
    "\n",
    "This is a rather quick and dirty kernel I created, with two ideas in mind: Training a \"2-headed\" network that will learn to predict siRNA using images from both sites at the same time, and split the learning process into two stages, namely first training on all data, then training the CNN on data from a single experiment at a time. The second idea comes from [this thread by Phalanx](https://www.kaggle.com/c/recursion-cellular-image-classification/discussion/100414#latest-586901). The data comes from my previous kernel on preprocessing.\n",
    "\n",
    "Here are the relevant sections:\n",
    "* **Data Generator**: The `__generate_X` method is pretty different, since it loads two images at the same time. Everything else is standard\n",
    "* **Model**: The CNN architecture used here is `EfficientNetB2`. With the right learning rates and enough time, you can probably try B1-B5; they have unfortunately not succeeded in my case. The inputs are two images, i.e. from site 1 and site 2. The two images are passed through the same CNN, then global-average-pooled, and added to form a single 1280-dimensional vector, which is ultimately used to perform predictions. This means that the networks will be updated simultaneously from the gradients of both sites.\n",
    "* **Phase 1**: Train the model on all data from 10 epochs, and save results to `model.h5`.\n",
    "* **Phase 2**: Load `model.h5` and train the model for 15 epochs on data from a single cell line, i.e. *HEPG2, HUVEC, RPE, U2OS*.\n",
    "\n",
    "## Changelog\n",
    "\n",
    "* V20: Added random flipping.\n",
    "* Data is taken from https://www.kaggle.com/xhlulu/recursion-2019-load-resize-and-save-images instead of the original data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/recursion-cellular-image-classification-224-jpg/train/train/HEPG2-06_2_O08_s1.jpeg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('../input/recursion-cellular-image-classification-224-jpg/train/train'): #/kaggle/input ../tmp/train_resized\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        break\n",
    "\n",
    "#/kaggle/input/recursion-2019-load-resize-and-save-images/test.zip\n",
    "#/kaggle/input/efficientnet-keras-weights-b0b5/efficientnet-b2_imagenet_1000_notop.h5\n",
    "#/kaggle/input/recursion-cellular-image-classification-224-jpg/new_test.csv\n",
    "#/kaggle/input/recursion-cellular-image-classification-224-jpg/test/test/HUVEC-23_4_F06_s2.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet\n",
      "  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in c:\\anaconda3\\lib\\site-packages (from efficientnet) (1.0.7)\n",
      "Requirement already satisfied: scikit-image in c:\\anaconda3\\lib\\site-packages (from efficientnet) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.16.4)\n",
      "Requirement already satisfied: h5py in c:\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.8.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (3.0.2)\n",
      "Requirement already satisfied: networkx>=1.8 in c:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (1.11)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (1.12.0)\n",
      "Requirement already satisfied: pillow>=4.3.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (5.3.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (1.0.1)\n",
      "Requirement already satisfied: dask[array]>=0.9.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in c:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (0.6.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->scikit-image->efficientnet) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->scikit-image->efficientnet) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->scikit-image->efficientnet) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->scikit-image->efficientnet) (2.7.5)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\anaconda3\\lib\\site-packages (from networkx>=1.8->scikit-image->efficientnet) (4.3.0)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in c:\\anaconda3\\lib\\site-packages (from dask[array]>=0.9.0->scikit-image->efficientnet) (0.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-image->efficientnet) (40.6.3)\n",
      "Installing collected packages: efficientnet\n",
      "Successfully installed efficientnet-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet\n",
    "import efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading https://files.pythonhosted.org/packages/cb/5a/abd74bd5ce791e2ab0b6fd88b144c42dbc88b3b1d963147417d0e163684b/scikit_image-0.16.2-cp37-cp37m-win_amd64.whl (25.7MB)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image) (5.3.0)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image) (2.4.1)\n",
      "Collecting networkx>=2.0 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl (1.6MB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.10.0 in c:\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in c:\\anaconda3\\lib\\site-packages (from networkx>=2.0->scikit-image) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (40.6.3)\n",
      "Installing collected packages: networkx, scikit-image\n",
      "  Found existing installation: networkx 1.11\n",
      "    Uninstalling networkx-1.11:\n",
      "      Successfully uninstalled networkx-1.11\n",
      "  Found existing installation: scikit-image 0.14.1\n",
      "    Uninstalling scikit-image-0.14.1:\n",
      "      Successfully uninstalled scikit-image-0.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-ldjxyx5_\\\\anaconda3\\\\lib\\\\site-packages\\\\skimage\\\\_shared\\\\geometry.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, concatenate, Input, add\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from efficientnet.keras import EfficientNetB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../tmp\n",
    "!unzip -q ../input/recursion-2019-load-resize-and-save-images/train.zip -d ../tmp\n",
    "!unzip -q ../input/recursion-2019-load-resize-and-save-images/test.zip -d ../tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 73030/73030 [10:38<00:00, 114.38it/s]\n"
     ]
    }
   ],
   "source": [
    "load_path = 'C:/Temp/recursion2/train'\n",
    "save_path = 'C:/Temp/recursion2/train_resized'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "for code in tqdm(os.listdir(load_path)):\n",
    "    path = f'{load_path}/{code}'\n",
    "    \n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (300, 300))\n",
    "    \n",
    "    cv2.imwrite(f'{save_path}/{code}', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73030, 7)\n",
      "(39794, 6)\n",
      "(73030, 1108)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>experiment</th>\n",
       "      <th>plate</th>\n",
       "      <th>well</th>\n",
       "      <th>sirna</th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HEPG2-01_1_B03</td>\n",
       "      <td>HEPG2-01</td>\n",
       "      <td>1</td>\n",
       "      <td>B03</td>\n",
       "      <td>513</td>\n",
       "      <td>HEPG2-01_1_B03_s1.jpeg</td>\n",
       "      <td>HEPG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HEPG2-01_1_B04</td>\n",
       "      <td>HEPG2-01</td>\n",
       "      <td>1</td>\n",
       "      <td>B04</td>\n",
       "      <td>840</td>\n",
       "      <td>HEPG2-01_1_B04_s1.jpeg</td>\n",
       "      <td>HEPG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HEPG2-01_1_B05</td>\n",
       "      <td>HEPG2-01</td>\n",
       "      <td>1</td>\n",
       "      <td>B05</td>\n",
       "      <td>1020</td>\n",
       "      <td>HEPG2-01_1_B05_s1.jpeg</td>\n",
       "      <td>HEPG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HEPG2-01_1_B06</td>\n",
       "      <td>HEPG2-01</td>\n",
       "      <td>1</td>\n",
       "      <td>B06</td>\n",
       "      <td>254</td>\n",
       "      <td>HEPG2-01_1_B06_s1.jpeg</td>\n",
       "      <td>HEPG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HEPG2-01_1_B07</td>\n",
       "      <td>HEPG2-01</td>\n",
       "      <td>1</td>\n",
       "      <td>B07</td>\n",
       "      <td>144</td>\n",
       "      <td>HEPG2-01_1_B07_s1.jpeg</td>\n",
       "      <td>HEPG2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id_code experiment  plate well  sirna                filename  \\\n",
       "0  HEPG2-01_1_B03   HEPG2-01      1  B03    513  HEPG2-01_1_B03_s1.jpeg   \n",
       "1  HEPG2-01_1_B04   HEPG2-01      1  B04    840  HEPG2-01_1_B04_s1.jpeg   \n",
       "2  HEPG2-01_1_B05   HEPG2-01      1  B05   1020  HEPG2-01_1_B05_s1.jpeg   \n",
       "3  HEPG2-01_1_B06   HEPG2-01      1  B06    254  HEPG2-01_1_B06_s1.jpeg   \n",
       "4  HEPG2-01_1_B07   HEPG2-01      1  B07    144  HEPG2-01_1_B07_s1.jpeg   \n",
       "\n",
       "  category  \n",
       "0    HEPG2  \n",
       "1    HEPG2  \n",
       "2    HEPG2  \n",
       "3    HEPG2  \n",
       "4    HEPG2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('C:/Temp/recursion2/new_train.csv')\n",
    "test_df = pd.read_csv('C:/Temp/recursion2/new_test.csv')\n",
    "\n",
    "train_df['category'] = train_df['experiment'].apply(lambda x: x.split('-')[0])\n",
    "test_df['category'] = test_df['experiment'].apply(lambda x: x.split('-')[0])\n",
    "\n",
    "train_target_df = pd.get_dummies(train_df['sirna'])\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(train_target_df.shape)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62075,)\n",
      "(10955,)\n"
     ]
    }
   ],
   "source": [
    "train_idx, val_idx = train_test_split(\n",
    "    train_df.index, test_size=0.15, random_state=2019\n",
    ")\n",
    "\n",
    "print(train_idx.shape)\n",
    "print(val_idx.shape)\n",
    "#(31037,)\n",
    "#(5478,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, df, target_df=None, mode='fit',\n",
    "                 base_path = 'C:/Temp/recursion2/train_resized',\n",
    "                 batch_size=32, dim=(224, 224), n_channels=3, ext='jpeg',\n",
    "                 rotation_range=0, fill_mode='nearest', swap=False,\n",
    "                 vertical_flip=False, horizontal_flip=False, rescale=1/255.,\n",
    "                 n_classes=5, random_state=2019, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df\n",
    "        self.mode = mode\n",
    "        self.base_path = base_path\n",
    "        self.rotation_range=rotation_range\n",
    "        self.target_df = target_df\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.ext = ext\n",
    "        self.rescale = rescale\n",
    "        self.vertical_flip = vertical_flip\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.random_state = random_state\n",
    "        self.swap = swap\n",
    "        \n",
    "        self.fill_mode = self.__compute_fill_mode(fill_mode)\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        X = self.__generate_X(list_IDs_batch)\n",
    "        \n",
    "        if self.mode == 'fit':\n",
    "            y = self.__generate_y(list_IDs_batch)\n",
    "            return X, y\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            return X\n",
    "        else:\n",
    "            raise AttributeError('The parameter mode should be set to \"fit\" or \"predict\".')\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __generate_X(self, list_IDs_batch):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X_1 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        X_2 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            code = self.df['id_code'].iloc[ID]\n",
    "            \n",
    "            img_path_1 = f\"{self.base_path}/{code}_s1.{self.ext}\"\n",
    "            img_path_2 = f\"{self.base_path}/{code}_s2.{self.ext}\"\n",
    "            \n",
    "            img1 = self.__load_image(img_path_1)\n",
    "            img2 = self.__load_image(img_path_2)\n",
    "            \n",
    "            if self.swap and np.random.rand() > 0.5:\n",
    "                img1, img2 = img2, img1\n",
    "            \n",
    "            # Store samples\n",
    "            X_1[i,] = img1\n",
    "            X_2[i,] = img2\n",
    "\n",
    "        return [X_1, X_2]\n",
    "    \n",
    "    def __generate_y(self, list_IDs_batch):\n",
    "        y = np.empty((self.batch_size, self.n_classes), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            sirna = self.target_df.iloc[ID]\n",
    "            y[i, ] = sirna\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def __load_image(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        img = self.rescale * img.astype(np.float32)\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def __compute_fill_mode(self, fill_mode):\n",
    "        convert_cv2 = {\n",
    "            'nearest': cv2.BORDER_REPLICATE,\n",
    "            'reflect': cv2.BORDER_REFLECT,\n",
    "            'wrap': cv2.BORDER_WRAP,\n",
    "            'constant': cv2.BORDER_CONSTANT\n",
    "        }\n",
    "        \n",
    "        return convert_cv2[fill_mode]\n",
    "    \n",
    "    def __random_transform(self, img):\n",
    "        if np.random.rand() > 0.5 and self.vertical_flip:\n",
    "            img = cv2.flip(img, 0)\n",
    "        if np.random.rand() > 0.5 and self.horizontal_flip:\n",
    "            img = cv2.flip(img, 1)\n",
    "        \n",
    "        # Random Rotation\n",
    "        rotation = self.rotation_range * np.random.rand()\n",
    "        \n",
    "        rows,cols = img.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((cols/2,rows/2),rotation,1)\n",
    "        img = cv2.warpAffine(img,M,(cols,rows), borderMode=self.fill_mode)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_generator = DataGenerator(\n",
    "    train_idx, \n",
    "    df=train_df,\n",
    "    target_df=train_target_df,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True,\n",
    "    swap=True,\n",
    "    dim=(300, 300),\n",
    "    base_path='C:/Temp/recursion2/train_resized',\n",
    "    rotation_range=15,\n",
    "    n_classes=train_target_df.shape[1]\n",
    ")\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    val_idx, \n",
    "    df=train_df,\n",
    "    target_df=train_target_df,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True,\n",
    "    swap=True,\n",
    "    dim=(300, 300),\n",
    "    base_path='C:/Temp/recursion2/train_resized',\n",
    "    rotation_range=15,\n",
    "    n_classes=train_target_df.shape[1]\n",
    ")\n",
    "\n",
    "test_generator = DataGenerator(\n",
    "    test_df.index, \n",
    "    df=test_df,\n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    mode='predict',\n",
    "    n_classes=train_target_df.shape[1],\n",
    "    dim=(300, 300),\n",
    "    base_path='C:/Temp/recursion2/test_resized'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36029, 42412, 23803, 60092, 20763, 20669, 53556, 44538, 60681,\n",
       "       37372, 43207, 20833, 26325, 39182, 32340, 32969, 58059, 36384,\n",
       "        3301, 13000, 47938, 22335, 61204, 17860, 38108, 11471, 56307,\n",
       "        7361, 45170, 34302,  6503,  8774])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_generator.__getitem__(0)\n",
    "indexes = train_generator.indexes[0:1*train_generator.batch_size]\n",
    "indexes\n",
    "# Find list of IDs\n",
    "#list_IDs_batch = [self.list_IDs[k] for k in indexes]\n",
    "#X = self.__generate_X(list_IDs_batch)\n",
    "list_IDs_batch = [train_generator.list_IDs[k] for k in indexes]\n",
    "list_IDs_batch[0] #45582\n",
    "#X = train_generator.testX([45582])\n",
    "code = train_generator.df['id_code'].iloc[45582]\n",
    "\n",
    "img_path = f\"{train_generator.base_path}/{code}_s1.{train_generator.ext}\"\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "#img = self.rescale * img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_classes, input_shape=(224, 224, 3)):\n",
    "    # First load mobilenet\n",
    "    backbone = EfficientNetB3(\n",
    "        weights='imagenet', \n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    im_inp_1 = Input(shape=input_shape)\n",
    "    im_inp_2 = Input(shape=input_shape)\n",
    "\n",
    "    x1 = backbone(im_inp_1)\n",
    "    x2 = backbone(im_inp_2)\n",
    "\n",
    "    x1 = GlobalAveragePooling2D()(x1)\n",
    "    x2 = GlobalAveragePooling2D()(x2)\n",
    "\n",
    "    out = add([x1, x2])\n",
    "    out = Dropout(0.5)(out)\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax')(out)\n",
    "\n",
    "    model = Model(inputs=[im_inp_1, im_inp_2], outputs=out)\n",
    "    \n",
    "    model.compile(Adam(0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 300, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 300, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "efficientnet-b3 (Model)         (None, 10, 10, 1536) 10783528    input_6[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 1536)         0           efficientnet-b3[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 1536)         0           efficientnet-b3[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1536)         0           global_average_pooling2d_3[0][0] \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1536)         0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1108)         1702996     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,486,524\n",
      "Trainable params: 12,399,228\n",
      "Non-trainable params: 87,296\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    input_shape=(300, 300, 3),\n",
    "    n_classes=train_target_df.shape[1]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-15e1ada6edb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m )\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3071\u001b[0m         \u001b[0mfeed_symbols\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_symbols\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetches\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3072\u001b[0m         session != self._session):\n\u001b[1;32m-> 3073\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[1;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[0;32m   3017\u001b[0m       \u001b[0mcallable_opts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m     \u001b[1;31m# Create callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3019\u001b[1;33m     \u001b[0mcallable_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3020\u001b[0m     \u001b[1;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m     \u001b[1;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[1;34m(self, callable_options)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \"\"\"\n\u001b[0;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1471\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session, callable_options)\u001b[0m\n\u001b[0;32m   1423\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[1;32m-> 1425\u001b[1;33m               session._session, options_ptr, status)\n\u001b[0m\u001b[0;32m   1426\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5', \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[checkpoint],\n",
    "    use_multiprocessing=False,\n",
    "    workers=1,\n",
    "    verbose=1,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "with open('history.json', 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df[['loss', 'val_loss']].plot()\n",
    "history_df[['acc', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: train on each cell line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = train_df['category'].unique()\n",
    "output_df = []\n",
    "\n",
    "for category in categories:\n",
    "    # Retrieve desired category\n",
    "    category_df = train_df[train_df['category'] == category]\n",
    "    cat_test_df = test_df[test_df['category'] == category].copy()\n",
    "    \n",
    "    print('\\n' + '=' * 40)\n",
    "    print(\"CURRENT CATEGORY:\", category)\n",
    "    print('-' * 40)\n",
    "    \n",
    "    train_idx, val_idx = train_test_split(\n",
    "        category_df.index, \n",
    "        random_state=2019,\n",
    "        test_size=0.15\n",
    "    )\n",
    "    \n",
    "    # Create new generators\n",
    "    train_generator = DataGenerator(\n",
    "        train_idx, \n",
    "        df=train_df,\n",
    "        target_df=train_target_df,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True,\n",
    "        swap=True,\n",
    "        rotation_range=15,\n",
    "        dim=(300, 300),\n",
    "        base_path='../tmp/train_resized',\n",
    "        n_classes=train_target_df.shape[1]\n",
    "    )\n",
    "\n",
    "    val_generator = DataGenerator(\n",
    "        val_idx, \n",
    "        df=train_df,\n",
    "        target_df=train_target_df,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True,\n",
    "        swap=True,\n",
    "        rotation_range=15,\n",
    "        dim=(300, 300),\n",
    "        base_path='../tmp/train_resized',\n",
    "        n_classes=train_target_df.shape[1]\n",
    "    )\n",
    "\n",
    "    test_generator = DataGenerator(\n",
    "        cat_test_df.index, \n",
    "        df=test_df,\n",
    "        batch_size=1, \n",
    "        shuffle=False,\n",
    "        mode='predict',\n",
    "        n_classes=train_target_df.shape[1],\n",
    "        dim=(300, 300),\n",
    "        base_path='../tmp/test_resized'\n",
    "    )\n",
    "\n",
    "    # Restore previously trained model\n",
    "    model.load_weights('model.h5')\n",
    "    model.compile(\n",
    "        Adam(0.0001), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Train model only on data for specific category\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f'model_{category}.h5', \n",
    "        monitor='val_loss', \n",
    "        verbose=0, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=False,\n",
    "        mode='auto'\n",
    "    )\n",
    "\n",
    "    history_category = model.fit_generator(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=[checkpoint],\n",
    "        use_multiprocessing=False,\n",
    "        workers=1,\n",
    "        verbose=2,\n",
    "        epochs=10\n",
    "    )\n",
    "\n",
    "    # Make prediction and add to output dataframe\n",
    "    y_pred = model.predict_generator(\n",
    "        test_generator,\n",
    "        workers=2,\n",
    "        use_multiprocessing=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    cat_test_df['sirna'] = y_pred.argmax(axis=1)\n",
    "    output_df.append(cat_test_df[['id_code', 'sirna']])\n",
    "\n",
    "    # Save history\n",
    "    with open(f'history_{category}.json', 'w') as f:\n",
    "        json.dump(history_category.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.concat(output_df)\n",
    "output_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
