Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 133, 133, 3) 0
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 133, 133, 3) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 62, 62, 256)  93184       input_1[0][0]
                                                                 input_2[0][0]
__________________________________________________________________________________________________
leaky_re_lu (LeakyReLU)         (None, 62, 62, 256)  0           conv2d[0][0]
                                                                 conv2d[1][0]
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 62, 62, 256)  1024        leaky_re_lu[0][0]
                                                                 leaky_re_lu[1][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 62, 62, 256)  0           batch_normalization[0][0]
                                                                 batch_normalization[1][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 58, 58, 256)  1638656     dropout[0][0]
                                                                 dropout[1][0]
__________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)       (None, 58, 58, 256)  0           conv2d_1[0][0]
                                                                 conv2d_1[1][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 29, 29, 256)  0           leaky_re_lu_1[0][0]
                                                                 leaky_re_lu_1[1][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 29, 29, 256)  1024        max_pooling2d[0][0]
                                                                 max_pooling2d[1][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 29, 29, 256)  0           batch_normalization_1[0][0]
                                                                 batch_normalization_1[1][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 25, 25, 128)  819328      dropout_1[0][0]
                                                                 dropout_1[1][0]
__________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)       (None, 25, 25, 128)  0           conv2d_2[0][0]
                                                                 conv2d_2[1][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 25, 25, 128)  512         leaky_re_lu_2[0][0]
                                                                 leaky_re_lu_2[1][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 23, 23, 128)  147584      batch_normalization_2[0][0]
                                                                 batch_normalization_2[1][0]
__________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)       (None, 23, 23, 128)  0           conv2d_3[0][0]
                                                                 conv2d_3[1][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 23, 23, 128)  512         leaky_re_lu_3[0][0]
                                                                 leaky_re_lu_3[1][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 21, 21, 64)   73792       batch_normalization_3[0][0]
                                                                 batch_normalization_3[1][0]
__________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)       (None, 21, 21, 64)   0           conv2d_4[0][0]
                                                                 conv2d_4[1][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 21, 21, 64)   256         leaky_re_lu_4[0][0]
                                                                 leaky_re_lu_4[1][0]
__________________________________________________________________________________________________
global_average_pooling2d (Globa (None, 64)           0           batch_normalization_4[0][0]
                                                                 batch_normalization_4[1][0]
__________________________________________________________________________________________________
dense (Dense)                   (None, 4096)         266240      global_average_pooling2d[0][0]
                                                                 global_average_pooling2d[1][0]
__________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)       (None, 4096)         0           dense[0][0]
                                                                 dense[1][0]
__________________________________________________________________________________________________
add (Add)                       (None, 4096)         0           leaky_re_lu_5[0][0]
                                                                 leaky_re_lu_5[1][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 2048)         8390656     add[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 2048)         8192        dense_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1024)         2098176     batch_normalization_5[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 1024)         4096        dense_2[0][0]
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 8)            8200        batch_normalization_6[0][0]
==================================================================================================
Total params: 13,551,432
Trainable params: 13,543,624
Non-trainable params: 7,808
__________________________________________________________________________________________________
Epoch 1/20
4562/4564 [============================>.] - ETA: 0s - loss: 61.3083 - acc: 0.1532
4564/4564 [==============================] - 265s 58ms/step - loss: 61.3061 - acc: 0.1532 - val_loss: 63.1142 - val_acc: 0.1948
Epoch 2/20
4563/4564 [============================>.] - ETA: 0s - loss: 62.8799 - acc: 0.1649
4564/4564 [==============================] - 259s 57ms/step - loss: 62.8796 - acc: 0.1649 - val_loss: 64.2038 - val_acc: 0.1761
Epoch 3/20
4563/4564 [============================>.] - ETA: 0s - loss: 63.9435 - acc: 0.2056
4564/4564 [==============================] - 259s 57ms/step - loss: 63.9404 - acc: 0.2056 - val_loss: 67.1295 - val_acc: 0.2088
Epoch 4/20
4562/4564 [============================>.] - ETA: 0s - loss: 66.1299 - acc: 0.1310
4564/4564 [==============================] - 259s 57ms/step - loss: 66.1232 - acc: 0.1310 - val_loss: 63.6023 - val_acc: 0.0997
Epoch 5/20
4563/4564 [============================>.] - ETA: 0s - loss: 67.0420 - acc: 0.1469
4564/4564 [==============================] - 259s 57ms/step - loss: 67.0431 - acc: 0.1469 - val_loss: 65.8754 - val_acc: 0.2076
Epoch 6/20
4563/4564 [============================>.] - ETA: 0s - loss: 68.0326 - acc: 0.1536
4564/4564 [==============================] - 259s 57ms/step - loss: 68.0318 - acc: 0.1536 - val_loss: 65.6975 - val_acc: 0.3787
Epoch 7/20
4563/4564 [============================>.] - ETA: 0s - loss: 67.6210 - acc: 0.1690
4564/4564 [==============================] - 259s 57ms/step - loss: 67.6222 - acc: 0.1691 - val_loss: 68.3155 - val_acc: 0.3882
Epoch 8/20
4563/4564 [============================>.] - ETA: 0s - loss: 68.0026 - acc: 0.1442
4564/4564 [==============================] - 259s 57ms/step - loss: 68.0057 - acc: 0.1442 - val_loss: 66.1181 - val_acc: 0.2076
Epoch 9/20
4563/4564 [============================>.] - ETA: 0s - loss: 66.2055 - acc: 0.2150
4564/4564 [==============================] - 259s 57ms/step - loss: 66.2057 - acc: 0.2150 - val_loss: 66.9672 - val_acc: 5.0261e-05
Epoch 10/20
4563/4564 [============================>.] - ETA: 0s - loss: 68.4298 - acc: 0.1745
4564/4564 [==============================] - 259s 57ms/step - loss: 68.4289 - acc: 0.1745 - val_loss: 70.8676 - val_acc: 0.0927
Epoch 11/20
4562/4564 [============================>.] - ETA: 0s - loss: 69.9614 - acc: 0.1977
4564/4564 [==============================] - 259s 57ms/step - loss: 69.9638 - acc: 0.1976 - val_loss: 74.4166 - val_acc: 0.0852
Epoch 12/20
4563/4564 [============================>.] - ETA: 0s - loss: 73.3476 - acc: 0.1938
4564/4564 [==============================] - 259s 57ms/step - loss: 73.3487 - acc: 0.1938 - val_loss: 80.9948 - val_acc: 0.1753
Epoch 13/20
4563/4564 [============================>.] - ETA: 0s - loss: 65.9819 - acc: 0.2077
4564/4564 [==============================] - 259s 57ms/step - loss: 65.9808 - acc: 0.2077 - val_loss: 67.4511 - val_acc: 0.2132
Epoch 14/20
4563/4564 [============================>.] - ETA: 0s - loss: 62.2909 - acc: 0.2111
4564/4564 [==============================] - 259s 57ms/step - loss: 62.2901 - acc: 0.2111 - val_loss: 62.8762 - val_acc: 0.4271
Epoch 15/20
4563/4564 [============================>.] - ETA: 0s - loss: 63.0035 - acc: 0.2040
4564/4564 [==============================] - 259s 57ms/step - loss: 63.0009 - acc: 0.2040 - val_loss: 62.2259 - val_acc: 0.1785
Epoch 16/20
4563/4564 [============================>.] - ETA: 0s - loss: 72.3380 - acc: 0.1848
4564/4564 [==============================] - 259s 57ms/step - loss: 72.3402 - acc: 0.1848 - val_loss: 88.5258 - val_acc: 0.1288
Epoch 17/20
4563/4564 [============================>.] - ETA: 0s - loss: 82.3858 - acc: 0.1591
4564/4564 [==============================] - 259s 57ms/step - loss: 82.3905 - acc: 0.1591 - val_loss: 92.6093 - val_acc: 0.1316
Epoch 18/20
4562/4564 [============================>.] - ETA: 0s - loss: 86.3078 - acc: 0.1383
4564/4564 [==============================] - 258s 56ms/step - loss: 86.3050 - acc: 0.1384 - val_loss: 167.4274 - val_acc: 0.2563
Epoch 19/20
4563/4564 [============================>.] - ETA: 0s - loss: 85.0120 - acc: 0.1269
4564/4564 [==============================] - 256s 56ms/step - loss: 85.0114 - acc: 0.1269 - val_loss: 113.2694 - val_acc: 0.0728
Epoch 20/20
4563/4564 [============================>.] - ETA: 0s - loss: 75.6253 - acc: 0.1358
4564/4564 [==============================] - 255s 56ms/step - loss: 75.6267 - acc: 0.1358 - val_loss: 4230.5800 - val_acc: 8.0418e-04
{'loss': [61.30612099118446, 62.87956848988713, 63.940366158665114, 66.12319661858415, 67.0431268718763, 68.03180714932374, 67.622220267                                                                                                     96533, 68.00573585286253, 66.20567821139939, 68.42886284879171, 69.96376532405014, 73.34872245412454, 65.98081338457848, 62.290133452436                                                                                                     365, 63.00085258651051, 72.34017365570135, 82.39047051980556, 86.30500111341685, 85.01138591933523, 75.62668778873346], 'acc': [0.153223                                                                                                     6, 0.1648773, 0.20564473, 0.13102542, 0.14686953, 0.15362073, 0.16906771, 0.14422655, 0.2150115, 0.17449057, 0.19760627, 0.19377191, 0.2                                                                                                     0773992, 0.21113606, 0.20402882, 0.18477488, 0.15907098, 0.13843393, 0.12690349, 0.13579097], 'val_loss': [63.11418324260765, 64.2038296                                                                                                     1615243, 67.12954991995977, 63.60231214016376, 65.87536890822143, 65.6975161157845, 68.31552466533249, 66.11810328886602, 66.96722192538                                                                                                     039, 70.86763486260145, 74.41657892862956, 80.9947785092589, 67.4510684586645, 62.87616838563337, 62.22590245613862, 88.52584821227856,                                                                                                      92.60932575251906, 167.42736752904656, 113.26935819588275, 4230.57997002197], 'val_acc': [0.19483815, 0.17606553, 0.20883594, 0.09971853                                                                                                     , 0.20762968, 0.37866908, 0.38824385, 0.20755428, 5.026136e-05, 0.09265681, 0.08516787, 0.17531163, 0.21318355, 0.4270959, 0.17847809, 0                                                                                                     .1287696, 0.1316345, 0.2563078, 0.07282871, 0.00080418173]}