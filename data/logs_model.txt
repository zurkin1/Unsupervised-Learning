Using TensorFlow backend.
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 133, 133, 3) 0
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 133, 133, 3) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 62, 62, 256)  93184       input_1[0][0]
                                                                 input_2[0][0]
__________________________________________________________________________________________________
leaky_re_lu (LeakyReLU)         (None, 62, 62, 256)  0           conv2d[0][0]
                                                                 conv2d[1][0]
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 62, 62, 256)  1024        leaky_re_lu[0][0]
                                                                 leaky_re_lu[1][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 62, 62, 256)  0           batch_normalization[0][0]
                                                                 batch_normalization[1][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 58, 58, 256)  1638656     dropout[0][0]
                                                                 dropout[1][0]
__________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)       (None, 58, 58, 256)  0           conv2d_1[0][0]
                                                                 conv2d_1[1][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 29, 29, 256)  0           leaky_re_lu_1[0][0]
                                                                 leaky_re_lu_1[1][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 29, 29, 256)  1024        max_pooling2d[0][0]
                                                                 max_pooling2d[1][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 29, 29, 256)  0           batch_normalization_1[0][0]
                                                                 batch_normalization_1[1][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 25, 25, 128)  819328      dropout_1[0][0]
                                                                 dropout_1[1][0]
__________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)       (None, 25, 25, 128)  0           conv2d_2[0][0]
                                                                 conv2d_2[1][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 25, 25, 128)  512         leaky_re_lu_2[0][0]
                                                                 leaky_re_lu_2[1][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 23, 23, 128)  147584      batch_normalization_2[0][0]
                                                                 batch_normalization_2[1][0]
__________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)       (None, 23, 23, 128)  0           conv2d_3[0][0]
                                                                 conv2d_3[1][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 23, 23, 128)  512         leaky_re_lu_3[0][0]
                                                                 leaky_re_lu_3[1][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 21, 21, 64)   73792       batch_normalization_3[0][0]
                                                                 batch_normalization_3[1][0]
__________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)       (None, 21, 21, 64)   0           conv2d_4[0][0]
                                                                 conv2d_4[1][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 21, 21, 64)   256         leaky_re_lu_4[0][0]
                                                                 leaky_re_lu_4[1][0]
__________________________________________________________________________________________________
global_average_pooling2d (Globa (None, 64)           0           batch_normalization_4[0][0]
                                                                 batch_normalization_4[1][0]
__________________________________________________________________________________________________
dense (Dense)                   (None, 4096)         266240      global_average_pooling2d[0][0]
                                                                 global_average_pooling2d[1][0]
__________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)       (None, 4096)         0           dense[0][0]
                                                                 dense[1][0]
__________________________________________________________________________________________________
add (Add)                       (None, 4096)         0           leaky_re_lu_5[0][0]
                                                                 leaky_re_lu_5[1][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 2048)         8390656     add[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 2048)         8192        dense_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1024)         2098176     batch_normalization_5[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 1024)         4096        dense_2[0][0]
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 4)            4100        batch_normalization_6[0][0]
==================================================================================================
Total params: 13,547,332
Trainable params: 13,539,524
Non-trainable params: 7,808
__________________________________________________________________________________________________
Epoch 1/20
WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/t                                                           ensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.pyt                                                           hon.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2186/2187 [============================>.] - ETA: 0s - loss: 16.7678 - accuracy: 0.2332
2187/2187 [==============================] - 225s 103ms/step - loss: 16.7677 - accuracy: 0.2331 -                                                            val_loss: 16.6808 - val_accuracy: 0.0829
Epoch 2/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6430 - accuracy: 0.2516
2187/2187 [==============================] - 212s 97ms/step - loss: 16.6430 - accuracy: 0.2516 - v                                                           al_loss: 16.6466 - val_accuracy: 0.2175
Epoch 3/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6439 - accuracy: 0.2320
2187/2187 [==============================] - 212s 97ms/step - loss: 16.6439 - accuracy: 0.2319 - v                                                           al_loss: 16.6375 - val_accuracy: 0.1244
Epoch 4/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6410 - accuracy: 0.2356
2187/2187 [==============================] - 211s 97ms/step - loss: 16.6410 - accuracy: 0.2355 - v                                                           al_loss: 16.6401 - val_accuracy: 0.4109
Epoch 5/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6438 - accuracy: 0.2294
2187/2187 [==============================] - 212s 97ms/step - loss: 16.6438 - accuracy: 0.2294 - v                                                           al_loss: 16.6362 - val_accuracy: 0.1270
Epoch 6/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6629 - accuracy: 0.2676
2187/2187 [==============================] - 211s 97ms/step - loss: 16.6629 - accuracy: 0.2677 - v                                                           al_loss: 16.6359 - val_accuracy: 0.1567
Epoch 7/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6407 - accuracy: 0.3151
2187/2187 [==============================] - 211s 96ms/step - loss: 16.6407 - accuracy: 0.3150 - v                                                           al_loss: 16.6828 - val_accuracy: 0.0935
Epoch 8/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6406 - accuracy: 0.2106
2187/2187 [==============================] - 211s 96ms/step - loss: 16.6406 - accuracy: 0.2106 - v                                                           al_loss: 16.6371 - val_accuracy: 8.0128e-04
Epoch 9/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6395 - accuracy: 0.2387
2187/2187 [==============================] - 211s 96ms/step - loss: 16.6395 - accuracy: 0.2386 - v                                                           al_loss: 16.6364 - val_accuracy: 0.0042
Epoch 10/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6382 - accuracy: 0.2430
2187/2187 [==============================] - 211s 97ms/step - loss: 16.6382 - accuracy: 0.2429 - v                                                           al_loss: 16.6366 - val_accuracy: 0.0607
Epoch 11/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6407 - accuracy: 0.2897
2187/2187 [==============================] - 211s 97ms/step - loss: 16.6407 - accuracy: 0.2896 - v                                                           al_loss: 16.6362 - val_accuracy: 0.8287
Epoch 12/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6394 - accuracy: 0.2764
2187/2187 [==============================] - 211s 97ms/step - loss: 16.6394 - accuracy: 0.2762 - v                                                           al_loss: 16.6365 - val_accuracy: 0.0445
Epoch 13/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6390 - accuracy: 0.2498
2187/2187 [==============================] - 211s 97ms/step - loss: 16.6390 - accuracy: 0.2497 - v                                                           al_loss: 16.6370 - val_accuracy: 0.7196
Epoch 14/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6377 - accuracy: 0.2684
2187/2187 [==============================] - 211s 97ms/step - loss: 16.6377 - accuracy: 0.2682 - v                                                           al_loss: 16.6382 - val_accuracy: 0.1254
Epoch 15/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6420 - accuracy: 0.2431
2187/2187 [==============================] - 212s 97ms/step - loss: 16.6420 - accuracy: 0.2430 - v                                                           al_loss: 16.6358 - val_accuracy: 6.0096e-04
Epoch 16/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6381 - accuracy: 0.2726
2187/2187 [==============================] - 212s 97ms/step - loss: 16.6380 - accuracy: 0.2728 - v                                                           al_loss: 16.6357 - val_accuracy: 0.2502
Epoch 17/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6391 - accuracy: 0.3056
2187/2187 [==============================] - 211s 97ms/step - loss: 16.6391 - accuracy: 0.3058 - v                                                           al_loss: 16.6361 - val_accuracy: 0.0803
Epoch 18/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6383 - accuracy: 0.3092
2187/2187 [==============================] - 212s 97ms/step - loss: 16.6383 - accuracy: 0.3094 - v                                                           al_loss: 16.6357 - val_accuracy: 0.8373
Epoch 19/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6373 - accuracy: 0.2485
2187/2187 [==============================] - 211s 96ms/step - loss: 16.6373 - accuracy: 0.2484 - v                                                           al_loss: 16.6403 - val_accuracy: 0.8884
Epoch 20/20
2186/2187 [============================>.] - ETA: 0s - loss: 16.6376 - accuracy: 0.2797
2187/2187 [==============================] - 211s 97ms/step - loss: 16.6376 - accuracy: 0.2797 - v                                                           al_loss: 16.6387 - val_accuracy: 0.0881
{'loss': [16.76770847357186, 16.643003764630016, 16.64392350087386, 16.64096203905619, 16.64376854                                                           9510536, 16.66286981318386, 16.64067292180735, 16.64062062539518, 16.639547237261933, 16.638217688                                                           80599, 16.64068714973858, 16.639427851366026, 16.639032139754153, 16.63768338350384, 16.6420227952                                                           5638, 16.638049661595364, 16.639089172705155, 16.63833367688284, 16.637291261579655, 16.6375716322                                                           35247], 'accuracy': [0.23313901, 0.25157177, 0.23191015, 0.23553956, 0.22936672, 0.26766118, 0.315                                                           04345, 0.21064815, 0.23862597, 0.24291267, 0.28958046, 0.27623457, 0.24974279, 0.26823273, 0.24296                                                           983, 0.2728052, 0.30581275, 0.309385, 0.24842821, 0.2796925], 'val_loss': [16.680821443215393, 16.                                                           64656755251762, 16.63745507215842, 16.6400860395187, 16.636173443916515, 16.63593077659607, 16.682                                                           763386995365, 16.637089882141506, 16.636352392343376, 16.636625815660526, 16.63624362456493, 16.63                                                           6458782049324, 16.637014327905117, 16.63822072591537, 16.635807336905064, 16.635672526481823, 16.6                                                           36127673662624, 16.635667709203865, 16.640349822166637, 16.63871612304296], 'val_accuracy': [0.082                                                           932696, 0.21754807, 0.124399036, 0.41085738, 0.12700321, 0.15665065, 0.093549676, 0.00080128206, 0                                                           .004206731, 0.060697116, 0.82872593, 0.044471152, 0.71955127, 0.12540065, 0.00060096156, 0.2502003                                                           3, 0.080328524, 0.83733976, 0.8884215, 0.088141024]}